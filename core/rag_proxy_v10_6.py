#!/usr/bin/env python3
"""
ğŸš€ ANIMARA RAG PROXY v10.6 â€” GOD MODE Ğ§Ğ•Ğ Ğ•Ğ— OPENAI SDK

ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ• v10.6:
God Mode Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ OpenAI SDK Ñ native function calling Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Codex CLI.

Ğ‘Ğ«Ğ›Ğ (v10.5): Codex CLI Ñ‡ĞµÑ€ĞµĞ· subprocess â€” Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ tools
Ğ¡Ğ¢ĞĞ›Ğ (v10.6): OpenAI SDK + native function calling â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ

ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° God Mode v10.6:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ğ—ĞĞŸĞ ĞĞ¡                                       â”‚
â”‚                      â”‚                                          â”‚
â”‚                      â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚    â”‚ ĞĞ‘Ğ©ĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ (Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²)        â”‚                 â”‚
â”‚    â”‚ â€¢ Workspace injection                    â”‚                 â”‚
â”‚    â”‚ â€¢ Hybrid Search (RAG)                    â”‚                 â”‚
â”‚    â”‚ â€¢ Session context                        â”‚                 â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                      â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚         â–¼                       â–¼                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ Ğ›ĞĞšĞĞ›Ğ¬ĞĞ â”‚          â”‚    GOD MODE       â”‚                â”‚
â”‚    â”‚  Qwen3   â”‚          â”‚   OpenAI SDK      â”‚                â”‚
â”‚    â”‚  :8010   â”‚          â”‚ + function callingâ”‚                â”‚
â”‚    â”‚ <tool>   â”‚          â”‚ + native tools    â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                       â”‚                              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                     â–¼                                          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚    â”‚ ĞĞ‘Ğ©ĞĞ¯ ĞŸĞĞ¡Ğ¢ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ                      â”‚                 â”‚
â”‚    â”‚ â€¢ Execute tools locally                  â”‚                 â”‚
â”‚    â”‚ â€¢ Save to session                        â”‚                 â”‚
â”‚    â”‚ â€¢ Extract facts                          â”‚                 â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import os
import re
import json
import time
import asyncio
import hashlib
import sys
from typing import Optional, Dict, List, Any
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from pathlib import Path

import httpx
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from sentence_transformers import SentenceTransformer
from pymilvus import MilvusClient
from rank_bm25 import BM25Okapi

# OpenAI SDK Ğ´Ğ»Ñ God Mode
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAI SDK not installed. Run: pip install openai")

sys.path.insert(0, os.path.expanduser("~/animara"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONFIG = {
    "version": "10.6",
    
    # Local LLM (Qwen3)
    "llm_api": "http://127.0.0.1:8010",
    "llm_model": "qwen3",
    "llm_max_tokens": 2000,
    "llm_context": 32768,
    
    # Milvus
    "milvus_uri": "http://localhost:19530",
    
    # Embedding
    "embedding_model": "/home/agx-thor/models/embeddings/bge-m3",
    
    # Paths
    "workspace_path": "/home/agx-thor/animara/workspace",
    "skills_path": "/home/agx-thor/animara/skills",
    
    # Users
    "default_person_id": "owner_sergey",
    "owner_person_id": "owner_sergey",
    
    # Cache
    "profile_cache_ttl": 300,
    
    # Session
    "session_max_messages": 20,
    "session_timeout": 1800,
    
    # Hybrid Search
    "vector_weight": 0.7,
    "bm25_weight": 0.3,
    "search_top_k": 5,
    
    # Memory Flush
    "context_limit": 32000,
    "flush_threshold": 28000,
    "reserve_tokens": 4000,
    
    # Session Pruning
    "prune_after_messages": 3,
    "prune_tool_max_chars": 200,
    
    # Tools
    "max_tool_iterations": 5,
    "tool_timeout": 30,
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GOD MODE â€” OpenAI SDK Ñ native function calling
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "openai_api_key": "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",
    "godmode_model": "gpt-4o-mini",  # Ğ”ĞµÑˆÑ‘Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ° ($0.15/1M input, $0.60/1M output)
    "godmode_max_tokens": 2000,
    "godmode_timeout": 120,
}

# Global objects
embedder = None
milvus = None
bm25_index = None
bm25_docs = []
bm25_ids = []
openai_client = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GOD MODE â€” OPENAI SDK Ğ¡ FUNCTION CALLING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ tools Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ OpenAI
OPENAI_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· Brave Search API. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ° Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ: Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, Ñ†ĞµĞ½Ñ‹, ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹, ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼"
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "yougile_tasks",
            "description": "ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· YouGile. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´ĞµĞ»Ğ°Ñ…, todo.",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "yougile_find",
            "description": "ĞĞ°Ğ¹Ñ‚Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² YouGile.",
            "parameters": {
                "type": "object",
                "properties": {
                    "search_term": {
                        "type": "string",
                        "description": "Ğ§Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°"
                    }
                },
                "required": ["search_term"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "yougile_create",
            "description": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² YouGile. ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾ÑÑÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ, ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ, Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"
                    },
                    "description": {
                        "type": "string",
                        "description": "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)"
                    }
                },
                "required": ["title"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "system_check",
            "description": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Docker ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹, Ğ´Ğ¸ÑĞº, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ.",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    }
]


def check_godmode_command(text: str) -> Optional[str]:
    """
    ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ God Mode.
    Returns: "activate", "deactivate", Ğ¸Ğ»Ğ¸ None
    """
    text_lower = text.lower().strip()
    
    activate_patterns = [
        r"^Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ²ĞºĞ»ÑÑ‡Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ²ĞºĞ»ÑÑ‡Ğ¸ Ğ±Ğ¾Ğ³Ğ°$",
        r"^/god$",
        r"^/godmode$",
        r"^godmode$",
        r"^god mode$",
        r"^god$",
    ]
    
    deactivate_patterns = [
        r"^Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ²Ñ‹ĞºĞ»ÑÑ‡Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ²Ñ‹ĞºĞ»ÑÑ‡Ğ¸ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ¸ Ğ±Ğ¾Ğ³Ğ°$",
        r"^Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼$",
        r"^/local$",
        r"^local$",
        r"^Ğ²Ñ‹Ñ…Ğ¾Ğ´$",
        r"^Ğ²Ñ‹Ğ¹Ğ´Ğ¸ Ğ¸Ğ· Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ±Ğ¾Ğ³Ğ°$",
    ]
    
    for pattern in activate_patterns:
        if re.match(pattern, text_lower):
            return "activate"
    
    for pattern in deactivate_patterns:
        if re.match(pattern, text_lower):
            return "deactivate"
    
    return None


async def call_godmode_llm(
    messages: List[dict], 
    system_prompt: str,
    tools_manager: "ToolsManager"
) -> dict:
    """
    Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ OpenAI API Ñ native function calling.
    
    ĞŸÑ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ“ĞĞ¢ĞĞ’Ğ«Ğ™ system_prompt (ÑƒĞ¶Ğµ Ñ Workspace + RAG + Session).
    ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ tool_calls Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ tools Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚.
    """
    global openai_client
    
    if not OPENAI_AVAILABLE:
        return {
            "choices": [{
                "message": {
                    "content": "âŒ OpenAI SDK Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸: `pip install openai`"
                }
            }],
            "error": "openai_not_installed"
        }
    
    if not openai_client:
        return {
            "choices": [{
                "message": {
                    "content": "âŒ OpenAI ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ½Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ API ĞºĞ»ÑÑ‡."
                }
            }],
            "error": "client_not_initialized"
        }
    
    try:
        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ messages Ğ´Ğ»Ñ OpenAI
        openai_messages = [{"role": "system", "content": system_prompt}]
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ user Ğ¸ assistant)
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role in ["user", "assistant"] and content:
                openai_messages.append({"role": role, "content": content})
        
        print(f"âš¡ Calling OpenAI ({CONFIG['godmode_model']})...")
        print(f"   Messages: {len(openai_messages)}, System prompt: {len(system_prompt)} chars")
        
        # ReAct loop Ğ´Ğ»Ñ God Mode
        for iteration in range(CONFIG["max_tool_iterations"]):
            print(f"âš¡ God Mode iteration {iteration + 1}")
            
            # Ğ’Ñ‹Ğ·Ğ¾Ğ² OpenAI Ñ tools
            response = await asyncio.to_thread(
                openai_client.chat.completions.create,
                model=CONFIG["godmode_model"],
                messages=openai_messages,
                tools=OPENAI_TOOLS,
                tool_choice="auto",
                max_tokens=CONFIG["godmode_max_tokens"],
                temperature=0.7,
            )
            
            assistant_message = response.choices[0].message
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ° tool calls
            if assistant_message.tool_calls:
                # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ tool call
                tool_results = []
                
                for tool_call in assistant_message.tool_calls:
                    tool_name = tool_call.function.name
                    try:
                        tool_args = json.loads(tool_call.function.arguments)
                    except json.JSONDecodeError:
                        tool_args = {}
                    
                    print(f"ğŸ”§ Tool call: {tool_name}({tool_args})")
                    
                    # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ tool Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾
                    if tools_manager:
                        tool_result = await tools_manager.execute_tool(tool_name, tool_args)
                    else:
                        tool_result = f"âŒ Tools manager Ğ½Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½"
                    
                    print(f"ğŸ“¤ Tool result: {tool_result[:100]}...")
                    
                    tool_results.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "content": tool_result
                    })
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ assistant message Ñ tool_calls
                openai_messages.append({
                    "role": "assistant",
                    "content": assistant_message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in assistant_message.tool_calls
                    ]
                })
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ tools
                for tr in tool_results:
                    openai_messages.append(tr)
                
                # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ loop Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
                continue
            
            else:
                # ĞĞµÑ‚ tool calls â€” ÑÑ‚Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
                final_content = assistant_message.content or ""
                
                return {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": f"âš¡ {final_content}"  # ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ´Ğ»Ñ God Mode
                        }
                    }],
                    "model": CONFIG["godmode_model"],
                    "god_mode": True,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                        "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                        "total_tokens": response.usage.total_tokens if response.usage else 0,
                    }
                }
        
        # Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
        return {
            "choices": [{
                "message": {
                    "content": "âš ï¸ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ God Mode. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ."
                }
            }],
            "error": "max_iterations"
        }
        
    except Exception as e:
        error_msg = str(e)
        print(f"âš ï¸ God Mode error: {error_msg}")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        if "api_key" in error_msg.lower() or "authentication" in error_msg.lower():
            return {
                "choices": [{
                    "message": {
                        "content": "âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ OpenAI. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ API ĞºĞ»ÑÑ‡ Ğ² CONFIG."
                    }
                }],
                "error": "auth_error"
            }
        
        if "rate_limit" in error_msg.lower():
            return {
                "choices": [{
                    "message": {
                        "content": "âŒ Rate limit OpenAI. ĞŸĞ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ Ğ¸ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ ÑĞ½Ğ¾Ğ²Ğ°."
                    }
                }],
                "error": "rate_limit"
            }
        
        return {
            "choices": [{
                "message": {
                    "content": f"âŒ God Mode Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {error_msg[:200]}"
                }
            }],
            "error": str(e)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOLS SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ToolsManager:
    """ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (skills)"""
    
    def __init__(self, skills_path: str):
        self.skills_path = Path(skills_path)
        self.tools = {}
        self._load_tools()
    
    def _load_tools(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ tools"""
        
        self.tools["web_search"] = {
            "name": "web_search",
            "description": "ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· Brave Search API.",
            "parameters": {"query": "ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"},
            "execute": self._execute_web_search
        }
        
        self.tools["yougile_tasks"] = {
            "name": "yougile_tasks",
            "description": "ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· YouGile.",
            "parameters": {},
            "execute": self._execute_yougile_tasks
        }
        
        self.tools["yougile_find"] = {
            "name": "yougile_find",
            "description": "ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² YouGile.",
            "parameters": {"search_term": "Ğ§Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ"},
            "execute": self._execute_yougile_find
        }
        
        self.tools["yougile_create"] = {
            "name": "yougile_create",
            "description": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² YouGile.",
            "parameters": {"title": "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ", "description": "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)"},
            "execute": self._execute_yougile_create
        }
        
        self.tools["system_check"] = {
            "name": "system_check",
            "description": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.",
            "parameters": {},
            "execute": self._execute_system_check
        }
        
        print(f"ğŸ”§ Loaded {len(self.tools)} tools: {list(self.tools.keys())}")
    
    async def _execute_web_search(self, params: dict) -> str:
        """Web search Ñ‡ĞµÑ€ĞµĞ· Brave API"""
        query = params.get("query", "")
        if not query:
            return "âŒ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"
        
        import requests
        api_key = "BSA1PthqtF-a8kZj7f_xNcLGBbMDfN3"
        
        try:
            response = requests.get(
                "https://api.search.brave.com/res/v1/web/search",
                headers={"Accept": "application/json", "X-Subscription-Token": api_key},
                params={"q": query, "count": 5},
                timeout=15
            )
            
            if response.status_code != 200:
                return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° API: {response.status_code}"
            
            results = response.json().get("web", {}).get("results", [])
            if not results:
                return f"ğŸ” ĞŸĞ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Â«{query}Â» Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾"
            
            output = []
            for i, item in enumerate(results[:5], 1):
                title = item.get("title", "")
                desc = item.get("description", "")[:200]
                url = item.get("url", "")
                output.append(f"{i}. {title}\n   {desc}\n   ğŸ”— {url}")
            
            return "\n\n".join(output)
        except Exception as e:
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
    
    async def _execute_yougile_tasks(self, params: dict) -> str:
        """Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡ YouGile"""
        import requests
        token = "eAbKs-KzViRbIzz+k0dscDYbfrUxJdlvC9OmeUN4YKZIxEt0gax9WUQpjbCB3wJg"
        
        try:
            response = requests.get(
                "https://ru.yougile.com/api-v2/tasks",
                headers={"Authorization": f"Bearer {token}"},
                timeout=15
            )
            
            if response.status_code != 200:
                return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° YouGile: {response.status_code}"
            
            tasks = response.json().get("content", [])
            active = [t for t in tasks[:15] if not t.get("deleted") and not t.get("completed")]
            
            if not active:
                return "ğŸ“‹ ĞĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡"
            
            output = []
            for t in active[:10]:
                output.append(f"â€¢ {t.get('title', 'Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ')}")
            
            return "ğŸ“‹ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸:\n" + "\n".join(output)
        except Exception as e:
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
    
    async def _execute_yougile_find(self, params: dict) -> str:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ YouGile"""
        import requests
        token = "eAbKs-KzViRbIzz+k0dscDYbfrUxJdlvC9OmeUN4YKZIxEt0gax9WUQpjbCB3wJg"
        search = params.get("search_term", "").lower()
        
        if not search:
            return "âŒ Ğ£ĞºĞ°Ğ¶Ğ¸ Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ"
        
        try:
            response = requests.get(
                "https://ru.yougile.com/api-v2/tasks",
                headers={"Authorization": f"Bearer {token}"},
                timeout=15
            )
            
            tasks = response.json().get("content", [])
            found = []
            
            for t in tasks:
                if t.get("deleted"):
                    continue
                if search in t.get("title", "").lower():
                    found.append(f"â€¢ {t.get('title')} (ID: {t.get('id')[:8]}...)")
            
            if not found:
                return f"ğŸ” Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Â«{search}Â» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°"
            
            return "ğŸ” ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾:\n" + "\n".join(found[:5])
        except Exception as e:
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
    
    async def _execute_yougile_create(self, params: dict) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ YouGile"""
        import requests
        token = "eAbKs-KzViRbIzz+k0dscDYbfrUxJdlvC9OmeUN4YKZIxEt0gax9WUQpjbCB3wJg"
        
        title = params.get("title", "")
        description = params.get("description", "")
        
        if not title:
            return "âŒ Ğ£ĞºĞ°Ğ¶Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"
        
        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ
            boards_resp = requests.get(
                "https://ru.yougile.com/api-v2/boards",
                headers={"Authorization": f"Bearer {token}"},
                timeout=15
            )
            boards = boards_resp.json().get("content", [])
            
            if not boards:
                return "âŒ ĞĞµÑ‚ Ğ´Ğ¾ÑĞ¾Ğº Ğ² YouGile"
            
            cols_resp = requests.get(
                f"https://ru.yougile.com/api-v2/columns?boardId={boards[0]['id']}",
                headers={"Authorization": f"Bearer {token}"},
                timeout=15
            )
            columns = cols_resp.json().get("content", [])
            
            if not columns:
                return "âŒ ĞĞµÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ½Ğ° Ğ´Ğ¾ÑĞºĞµ"
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ
            payload = {"title": title, "columnId": columns[0]["id"]}
            if description:
                payload["description"] = description
            
            create_resp = requests.post(
                "https://ru.yougile.com/api-v2/tasks",
                headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json"},
                json=payload,
                timeout=15
            )
            
            if create_resp.status_code in [200, 201]:
                task_id = create_resp.json().get("id", "")[:8]
                return f"âœ… Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°: Â«{title}Â» (ID: {task_id}...)"
            else:
                return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ: {create_resp.status_code}"
        except Exception as e:
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"
    
    async def _execute_system_check(self, params: dict) -> str:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        import subprocess
        
        output = ["ğŸ–¥ï¸ **Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹:**"]
        
        # Docker
        try:
            result = subprocess.run(
                ["docker", "ps", "--format", "{{.Names}}: {{.Status}}"],
                capture_output=True, text=True, timeout=10
            )
            containers = result.stdout.strip().split("\n")[:5]
            output.append("\n**Docker:**")
            for c in containers:
                if c:
                    output.append(f"  â€¢ {c}")
        except:
            output.append("  âš ï¸ Docker Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
        
        # Ğ”Ğ¸ÑĞº
        try:
            result = subprocess.run(
                ["df", "-h", "/media/agx-thor/SSD_AI"],
                capture_output=True, text=True, timeout=5
            )
            lines = result.stdout.strip().split("\n")
            if len(lines) > 1:
                parts = lines[1].split()
                output.append(f"\n**SSD:** {parts[2]} / {parts[1]} ({parts[4]} Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¾)")
        except:
            output.append("\n**SSD:** âš ï¸ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
        
        return "\n".join(output)
    
    def get_tools_prompt(self) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ ÑĞ¾ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ LLM"""
        lines = ["Ğ”ĞĞ¡Ğ¢Ğ£ĞŸĞĞ«Ğ• Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢Ğ«:"]
        lines.append('Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚, Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸: <tool>{"name": "Ğ¸Ğ¼Ñ", "params": {...}}</tool>')
        lines.append("")
        
        for name, tool in self.tools.items():
            params = ", ".join(f'{k}: "{v}"' for k, v in tool["parameters"].items())
            lines.append(f"â€¢ {name}({params}) â€” {tool['description']}")
        
        lines.append("")
        lines.append("Ğ’ĞĞ–ĞĞ: ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° â€” Ğ´Ğ°Ğ¹ ĞšĞ ĞĞ¢ĞšĞ˜Ğ™ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ!")
        
        return "\n".join(lines)
    
    async def execute_tool(self, name: str, params: dict) -> str:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚"""
        if name not in self.tools:
            return f"âŒ ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚: {name}"
        
        tool = self.tools[name]
        try:
            result = await asyncio.wait_for(
                tool["execute"](params),
                timeout=CONFIG["tool_timeout"]
            )
            return result
        except asyncio.TimeoutError:
            return f"âŒ Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ {name}"
        except Exception as e:
            return f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° {name}: {e}"

tools_manager = None


def parse_tool_call(text: str) -> Optional[dict]:
    """
    ĞŸĞ°Ñ€ÑĞ¸Ñ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ LLM.
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° (Qwen3).
    God Mode Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ native function calling.
    """
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 1: <tool>JSON</tool>
    match = re.search(r'<tool>\s*(\{.*?\})\s*</tool>', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except:
            pass
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 2: function_name() Ğ¸Ğ»Ğ¸ function_name("param")
    tool_patterns = [
        (r'yougile_tasks\s*\(\s*\)', {"name": "yougile_tasks", "params": {}}),
        (r'yougile_find\s*\(\s*["\']([^"\']+)["\']\s*\)', lambda m: {"name": "yougile_find", "params": {"search_term": m.group(1)}}),
        (r'yougile_create\s*\(\s*["\']([^"\']+)["\']\s*\)', lambda m: {"name": "yougile_create", "params": {"title": m.group(1)}}),
        (r'web_search\s*\(\s*["\']([^"\']+)["\']\s*\)', lambda m: {"name": "web_search", "params": {"query": m.group(1)}}),
        (r'system_check\s*\(\s*\)', {"name": "system_check", "params": {}}),
    ]
    
    for pattern, result in tool_patterns:
        match = re.search(pattern, text)
        if match:
            if callable(result):
                return result(match)
            return result
    
    return None


def needs_thinking(text: str) -> bool:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½ÑƒĞ¶ĞµĞ½ Ğ»Ğ¸ thinking mode Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ LLM"""
    text_lower = text.lower()
    
    patterns = [
        r'\d+\s*[\+\-\*\/\%]\s*\d+',
        r'ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ±ÑƒĞ´ĞµÑ‚', r'Ğ¿Ğ¾ÑÑ‡Ğ¸Ñ‚Ğ°Ğ¹', r'Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸', r'Ñ€ĞµÑˆĞ¸',
        r'Ğ·Ğ°Ğ´Ğ°Ñ‡[Ğ°Ğ¸]', r'Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğº', r'Ğ²Ğ¾Ğ»Ğº.*ĞºĞ¾Ğ·.*ĞºĞ°Ğ¿ÑƒÑÑ‚',
        r'Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ ĞºĞ¾Ğ´', r'Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸', r'Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼',
        r'Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹', r'ÑÑ€Ğ°Ğ²Ğ½Ğ¸', r'Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ',
        r'ÑĞ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ¿Ğ»Ğ°Ğ½', r'Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²',
    ]
    
    return any(re.search(p, text_lower) for p in patterns)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOKEN COUNTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def count_tokens(text: str) -> int:
    return len(text) // 3 if text else 0


def count_messages_tokens(messages: List[dict]) -> int:
    return sum(count_tokens(m.get("content", "")) + 4 for m in messages)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKSPACE LOADER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class WorkspaceLoader:
    def __init__(self, workspace_path: str):
        self.workspace = Path(workspace_path)
        self.memory_dir = self.workspace / "memory"
        self.cache = {}
        self.cache_time = 0
        self.cache_ttl = 60
    
    def _read_file(self, filename: str) -> Optional[str]:
        path = self.workspace / filename
        if path.exists():
            try:
                return path.read_text(encoding='utf-8')[:4000]
            except:
                return None
        return None
    
    def _read_memory_file(self, date_str: str) -> Optional[str]:
        path = self.memory_dir / f"{date_str}.md"
        if path.exists():
            try:
                return path.read_text(encoding='utf-8')[:2000]
            except:
                return None
        return None
    
    def get_context(self) -> str:
        now = time.time()
        if self.cache and (now - self.cache_time) < self.cache_ttl:
            return self.cache.get("context", "")
        
        parts = []
        for f in ["SOUL.md", "IDENTITY.md", "OWNER.md", "MEMORY.md", "TOOLS.md"]:
            content = self._read_file(f)
            if content:
                parts.append(content)
        
        today = datetime.now().strftime("%Y-%m-%d")
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        
        for date in [today, yesterday]:
            content = self._read_memory_file(date)
            if content:
                parts.append(f"<!-- {date} -->\n{content}")
        
        context = "\n\n---\n\n".join(parts)
        self.cache = {"context": context}
        self.cache_time = now
        return context
    
    def write_memory(self, content: str) -> bool:
        today = datetime.now().strftime("%Y-%m-%d")
        path = self.memory_dir / f"{today}.md"
        
        try:
            self.memory_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%H:%M")
            
            if path.exists():
                existing = path.read_text(encoding='utf-8')
                new_content = f"{existing}\n\n## [{timestamp}] Memory Flush\n\n{content}"
            else:
                new_content = f"# ğŸ“… {today}\n\n## [{timestamp}] Memory Flush\n\n{content}"
            
            path.write_text(new_content, encoding='utf-8')
            self.cache = {}
            print(f"ğŸ’¾ Memory flushed to {path}")
            return True
        except Exception as e:
            print(f"âš ï¸ Write memory error: {e}")
            return False
    
    def invalidate_cache(self):
        self.cache = {}

workspace = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BM25 INDEX
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tokenize_ru(text: str) -> List[str]:
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    return [w for w in text.split() if len(w) > 2]


def build_bm25_index():
    global bm25_index, bm25_docs, bm25_ids
    print("ğŸ“š Building BM25 index...")
    
    docs, ids = [], []
    try:
        memories = milvus.query("memories", filter="is_active == true",
                                output_fields=["id", "content"], limit=1000)
        for m in memories:
            docs.append(m.get("content", ""))
            ids.append(f"mem_{m.get('id')}")
    except Exception as e:
        print(f"âš ï¸ BM25 memories error: {e}")
    
    try:
        convs = milvus.query("conversations", filter="",
                             output_fields=["id", "content"], limit=500)
        for c in convs:
            docs.append(c.get("content", ""))
            ids.append(f"conv_{c.get('id')}")
    except Exception as e:
        print(f"âš ï¸ BM25 conversations error: {e}")
    
    if docs:
        tokenized = [tokenize_ru(d) for d in docs]
        bm25_index = BM25Okapi(tokenized)
        bm25_docs = docs
        bm25_ids = ids
        print(f"ğŸ“š BM25 index: {len(docs)} documents")
    else:
        bm25_index = None
        bm25_docs = []
        bm25_ids = []
        print("âš ï¸ BM25 index empty")


def bm25_search(query: str, top_k: int = 5) -> List[tuple]:
    global bm25_index, bm25_docs, bm25_ids
    
    if not bm25_index or not bm25_docs:
        return []
    
    tokens = tokenize_ru(query)
    if not tokens:
        return []
    
    scores = bm25_index.get_scores(tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    
    results = []
    for i in top_indices:
        if scores[i] > 0:
            results.append((bm25_docs[i], scores[i], bm25_ids[i]))
    
    return results


def hybrid_search(query: str, person_id: str, top_k: int = 5) -> List[str]:
    """Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: Vector + BM25"""
    global embedder, milvus
    
    results = {}
    
    # Vector search
    try:
        query_vector = embedder.encode([query])[0].tolist()
        
        # Memories
        mem_results = milvus.search(
            collection_name="memories",
            data=[query_vector],
            filter=f'is_active == true && person_id == "{person_id}"',
            limit=top_k,
            output_fields=["content"]
        )
        
        for hits in mem_results:
            for hit in hits:
                content = hit["entity"].get("content", "")
                if content:
                    score = 1 - hit["distance"] if hit["distance"] < 1 else 0
                    results[content] = results.get(content, 0) + score * CONFIG["vector_weight"]
        
        # Conversations
        conv_results = milvus.search(
            collection_name="conversations",
            data=[query_vector],
            filter=f'person_id == "{person_id}"',
            limit=top_k,
            output_fields=["content"]
        )
        
        for hits in conv_results:
            for hit in hits:
                content = hit["entity"].get("content", "")
                if content:
                    score = 1 - hit["distance"] if hit["distance"] < 1 else 0
                    results[content] = results.get(content, 0) + score * CONFIG["vector_weight"] * 0.5
    except Exception as e:
        print(f"âš ï¸ Vector search error: {e}")
    
    # BM25 only for owner (security)
    if person_id == CONFIG["owner_person_id"]:
        bm25_results = bm25_search(query, top_k * 2)
        if bm25_results:
            max_bm25 = max(r[1] for r in bm25_results)
            for content, score, _ in bm25_results:
                normalized = score / max_bm25 if max_bm25 > 0 else 0
                results[content] = results.get(content, 0) + normalized * CONFIG["bm25_weight"]
    
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    return [content for content, score in sorted_results[:top_k]]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SESSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Session:
    def __init__(self, person_id: str):
        self.person_id = person_id
        self.session_id = f"s_{int(time.time())}_{hashlib.md5(os.urandom(4)).hexdigest()[:6]}"
        self.messages: List[dict] = []
        self.created_at = time.time()
        self.last_activity = time.time()
        self.facts_extracted: List[str] = []
        self.total_tokens = 0
        self.flush_done = False
        self.tool_calls = 0
        # GOD MODE
        self.god_mode = False
    
    def add_message(self, role: str, content: str, is_tool: bool = False):
        tokens = count_tokens(content)
        self.messages.append({
            "role": role, 
            "content": content, 
            "ts": time.time(), 
            "tokens": tokens,
            "is_tool": is_tool
        })
        self.last_activity = time.time()
        self.total_tokens += tokens
        
        self._prune_old_tool_results()
        
        if len(self.messages) > CONFIG["session_max_messages"]:
            removed = self.messages.pop(0)
            self.total_tokens -= removed.get("tokens", 0)
    
    def _prune_old_tool_results(self):
        assistant_count = 0
        prune_before_idx = -1
        
        for i in range(len(self.messages) - 1, -1, -1):
            if self.messages[i]["role"] == "assistant":
                assistant_count += 1
                if assistant_count >= CONFIG["prune_after_messages"]:
                    prune_before_idx = i
                    break
        
        if prune_before_idx <= 0:
            return
        
        tokens_saved = 0
        for i in range(prune_before_idx):
            msg = self.messages[i]
            if msg.get("is_tool") and len(msg["content"]) > CONFIG["prune_tool_max_chars"]:
                old_tokens = msg["tokens"]
                msg["content"] = msg["content"][:CONFIG["prune_tool_max_chars"]] + "... [pruned]"
                msg["tokens"] = count_tokens(msg["content"])
                tokens_saved += old_tokens - msg["tokens"]
        
        if tokens_saved > 0:
            self.total_tokens -= tokens_saved
    
    def get_context(self, max_messages: int = 6) -> str:
        if not self.messages:
            return ""
        lines = []
        for msg in self.messages[-max_messages:]:
            role = "Animara" if msg["role"] == "assistant" else "User"
            content = msg["content"][:300] + "..." if len(msg["content"]) > 300 else msg["content"]
            lines.append(f"{role}: {content}")
        return "\n".join(lines)
    
    def is_expired(self) -> bool:
        return time.time() - self.last_activity > CONFIG["session_timeout"]
    
    def needs_flush(self) -> bool:
        return self.total_tokens > CONFIG["flush_threshold"]
    
    def compact(self):
        self.messages = self.messages[-3:]
        self.total_tokens = sum(m.get("tokens", 0) for m in self.messages)
    
    def get_stats(self) -> dict:
        return {
            "session_id": self.session_id,
            "person_id": self.person_id,
            "messages": len(self.messages),
            "total_tokens": self.total_tokens,
            "flush_threshold": CONFIG["flush_threshold"],
            "needs_flush": self.needs_flush(),
            "flush_done": self.flush_done,
            "tool_calls": self.tool_calls,
            "god_mode": self.god_mode,
        }


class SessionManager:
    def __init__(self):
        self.sessions: Dict[str, Session] = {}
    
    def get_or_create(self, person_id: str) -> Session:
        if person_id in self.sessions:
            session = self.sessions[person_id]
            if session.is_expired():
                print(f"â™»ï¸ Session expired for {person_id}, creating new")
                session = Session(person_id)
                self.sessions[person_id] = session
            return session
        
        session = Session(person_id)
        self.sessions[person_id] = session
        print(f"ğŸ†• New session for {person_id}: {session.session_id}")
        return session
    
    def end_session(self, person_id: str):
        if person_id in self.sessions:
            del self.sessions[person_id]
            print(f"ğŸ”š Session ended for {person_id}")
    
    async def memory_flush(self, session: Session):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ ÑĞµÑÑĞ¸Ğ¸"""
        if not session.messages:
            return
        
        context = session.get_context(10)
        
        flush_prompt = f"""ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸ 3-7 Ğ’ĞĞ–ĞĞ«Ğ¥ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.

Ğ”Ğ˜ĞĞ›ĞĞ“:
{context}

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº:
â€¢ Ğ¤Ğ°ĞºÑ‚ 1
â€¢ Ğ¤Ğ°ĞºÑ‚ 2
..."""
        
        try:
            async with httpx.AsyncClient(timeout=60) as client:
                resp = await client.post(
                    f"{CONFIG['llm_api']}/v1/chat/completions",
                    json={
                        "model": CONFIG["llm_model"],
                        "messages": [{"role": "user", "content": flush_prompt}],
                        "max_tokens": 500,
                        "temperature": 0.3,
                    }
                )
                
                if resp.status_code == 200:
                    facts = resp.json().get("choices", [{}])[0].get("message", {}).get("content", "")
                    if facts:
                        workspace.write_memory(facts)
                        session.flush_done = True
                        session.compact()
                        print(f"ğŸ’¾ Memory flushed for {session.person_id}")
        except Exception as e:
            print(f"âš ï¸ Memory flush error: {e}")

session_manager = SessionManager()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FACT EXTRACTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FACT_PATTERNS = [
    (r"Ğ¼ĞµĞ½Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚\s+([Ğ-Ğ¯Ğ°-ÑA-Za-z]+)", "fact", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚ {0}"),
    (r"Ñ Ğ¶Ğ¸Ğ²Ñƒ\s+(?:Ğ²|Ğ½Ğ°)\s+(.+?)(?:\.|,|$)", "fact", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¶Ğ¸Ğ²Ñ‘Ñ‚ Ğ² {0}"),
    (r"Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ\s+(.+?)(?:\.|,|$)", "fact", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ {0}"),
    (r"Ñ Ğ»ÑĞ±Ğ»Ñ\s+(.+?)(?:\.|,|$)", "preference", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ»ÑĞ±Ğ¸Ñ‚ {0}"),
    (r"Ğ¼Ğ½Ğµ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ\s+(.+?)(?:\.|,|$)", "preference", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ {0}"),
    (r"Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚\s+(.+?)(?:\.|,|$)", "project", "ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {0}"),
    (r"Ñ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑÑŒ\s+(.+?)(?:\.|,|!|$)", "hobby", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ {0}"),
    (r"Ñ ÑƒĞ²Ğ»ĞµĞºĞ°ÑÑÑŒ\s+(.+?)(?:\.|,|$)", "hobby", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒĞ²Ğ»ĞµĞºĞ°ĞµÑ‚ÑÑ {0}"),
    (r"Ñ ÑƒĞ¼ĞµÑ\s+(.+?)(?:\.|,|$)", "skill", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒĞ¼ĞµĞµÑ‚ {0}"),
    (r"Ñ Ñ…Ğ¾Ñ‡Ñƒ\s+(.+?)(?:\.|,|$)", "plan", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ…Ğ¾Ñ‡ĞµÑ‚ {0}"),
    (r"Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑ\s+(.+?)(?:\.|,|$)", "plan", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ {0}"),
]


def extract_and_save_facts(text: str, person_id: str, session: Session):
    """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² Milvus"""
    global embedder, milvus
    
    if not text or len(text) < 10:
        return
    
    text_lower = text.lower()
    
    for pattern, fact_type, template in FACT_PATTERNS:
        match = re.search(pattern, text_lower)
        if match:
            try:
                fact_text = template.format(*match.groups())
                
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
                if fact_text in session.facts_extracted:
                    continue
                
                session.facts_extracted.append(fact_text)
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Milvus
                embedding = embedder.encode([fact_text])[0].tolist()
                
                milvus.insert(
                    collection_name="memories",
                    data=[{
                        "person_id": person_id,
                        "memory_type": fact_type,
                        "content": fact_text,
                        "content_embedding": embedding,
                        "confidence": 0.8,
                        "source_session_id": session.session_id,
                        "is_active": True,
                        "created_at": int(time.time()),
                        "updated_at": int(time.time()),
                    }]
                )
                
                print(f"ğŸ’¡ Fact saved: {fact_text[:50]}...")
            except Exception as e:
                print(f"âš ï¸ Fact save error: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FASTAPI APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@asynccontextmanager
async def lifespan(app: FastAPI):
    global embedder, milvus, workspace, tools_manager, openai_client
    
    print(f"ğŸš€ Starting Animara RAG Proxy v{CONFIG['version']}...")
    
    # Embedder
    print("ğŸ“¦ Loading BGE-M3...")
    embedder = SentenceTransformer(CONFIG["embedding_model"])
    print("âœ… Embedder loaded")
    
    # Milvus
    print("ğŸ”Œ Connecting to Milvus...")
    milvus = MilvusClient(uri=CONFIG["milvus_uri"])
    print(f"âœ… Milvus connected: {milvus.list_collections()}")
    
    # BM25
    build_bm25_index()
    
    # Workspace
    workspace = WorkspaceLoader(CONFIG["workspace_path"])
    print(f"ğŸ“ Workspace loaded: {len(workspace.get_context())} chars")
    
    # Tools
    tools_manager = ToolsManager(CONFIG["skills_path"])
    
    # OpenAI client Ğ´Ğ»Ñ God Mode
    if OPENAI_AVAILABLE and CONFIG.get("openai_api_key"):
        openai_client = OpenAI(api_key=CONFIG["openai_api_key"])
        print(f"âš¡ OpenAI client initialized (model: {CONFIG['godmode_model']})")
    else:
        print("âš ï¸ OpenAI client NOT initialized (no SDK or API key)")
    
    print(f"âœ… Animara RAG Proxy v{CONFIG['version']} ready!")
    
    yield
    
    print("ğŸ‘‹ Shutting down...")

app = FastAPI(title="Animara RAG Proxy", version=CONFIG["version"], lifespan=lifespan)


@app.get("/health")
async def health():
    return {
        "status": "ok",
        "version": CONFIG["version"],
        "features": ["workspace", "hybrid_search", "bm25", "memory_flush", 
                     "session_pruning", "TOOLS", "THINKING_MODE", "GOD_MODE_OPENAI_SDK"],
        "godmode": {
            "model": CONFIG["godmode_model"],
            "openai_available": OPENAI_AVAILABLE,
            "client_initialized": openai_client is not None,
        },
        "llm": CONFIG["llm_model"],
        "milvus_collections": milvus.list_collections() if milvus else [],
        "bm25_docs": len(bm25_docs),
        "active_sessions": len(session_manager.sessions),
    }


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ ENDPOINT â€” Ğ•Ğ”Ğ˜ĞĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ Ğ”Ğ›Ğ¯ ĞĞ‘ĞĞ˜Ğ¥ Ğ Ğ•Ğ–Ğ˜ĞœĞĞ’
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    God Mode Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞĞ”Ğ˜ĞĞĞšĞĞ’Ğ£Ğ® Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ:
    1. Workspace injection
    2. Hybrid Search (RAG)
    3. Session context
    
    Ğ Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ² LLM backend Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ tools:
    - Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹: Qwen3 + <tool>JSON</tool>
    - God Mode: OpenAI SDK + native function calling
    """
    body = await request.json()
    
    messages = body.get("messages", [])
    person_id = body.get("person_id", CONFIG["default_person_id"])
    enable_tools = body.get("enable_tools", True)
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞµÑÑĞ¸Ñ
    session = session_manager.get_or_create(person_id)
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ user message
    user_message = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            user_message = msg.get("content", "")
            break
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GOD MODE COMMANDS (activate/deactivate)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    godmode_cmd = check_godmode_command(user_message)
    
    if godmode_cmd == "activate":
        # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ owner Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ God Mode
        if person_id != CONFIG["owner_person_id"]:
            return {
                "choices": [{"message": {"content": "âŒ God Mode Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñƒ."}}],
                "animara_stats": {"session": session.get_stats()}
            }
        
        session.god_mode = True
        
        return {
            "choices": [{
                "message": {
                    "content": f"""âš¡ **Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ‘Ğ¾Ğ³Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½!**

ğŸ§  **ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** {CONFIG['godmode_model']}
ğŸ”§ **OpenAI SDK:** {'âœ… Ğ³Ğ¾Ñ‚Ğ¾Ğ²' if openai_client else 'âŒ Ğ½Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½'}
ğŸ“Š **ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:** ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ (Workspace + RAG + Session)
ğŸ› ï¸ **Tools:** Native function calling

**Ğ§Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¾ÑÑŒ:**
â€¢ LLM backend: Qwen3 â†’ OpenAI ({CONFIG['godmode_model']})
â€¢ Tools: <tool>JSON</tool> â†’ native function calling
â€¢ Ğ’ÑÑ‘ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ (Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚) â€” Ğ‘Ğ•Ğ— Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ™

**ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:**
â€¢ `/local` â€” Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Qwen3"""
                }
            }],
            "animara_stats": {"session": session.get_stats(), "god_mode": True}
        }
    
    if godmode_cmd == "deactivate":
        session.god_mode = False
        return {
            "choices": [{
                "message": {
                    "content": f"""âœ… **Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½!**

ğŸ§  **ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** {CONFIG['llm_model']} (Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹)
ğŸ“Š **ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:** 32K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²
ğŸ’° **Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ:** $0

**ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°:** "Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ±Ğ¾Ğ³Ğ°" Ğ¸Ğ»Ğ¸ `/god` â€” Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ÑĞ½Ğ¾Ğ²Ğ°"""
                }
            }],
            "animara_stats": {"session": session.get_stats(), "god_mode": False}
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞĞ‘Ğ©ĞĞ¯ Ğ›ĞĞ“Ğ˜ĞšĞ (Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # === 1. WORKSPACE ===
    if person_id != CONFIG["owner_person_id"]:
        workspace_ctx = "Ğ¢Ñ‹ â€” Animara, AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²ÑŒÑÑ Ğ¸ ÑĞ¿Ñ€Ğ¾ÑĞ¸ Ñ‡ĞµĞ¼ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ."
    else:
        workspace_ctx = workspace.get_context()
    
    # === 2. HYBRID SEARCH (RAG) ===
    rag_context = ""
    if user_message and ("?" in user_message or any(w in user_message.lower() 
        for w in ["Ñ‡Ñ‚Ğ¾", "ĞºĞ°Ğº", "Ğ³Ğ´Ğµ", "ĞºĞ¾Ğ³Ğ´Ğ°", "Ğ¿Ğ¾Ğ¼Ğ½Ğ¸ÑˆÑŒ", "Ğ·Ğ½Ğ°ĞµÑˆÑŒ", "Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ğ¸"])):
        relevant = hybrid_search(user_message, person_id, CONFIG["search_top_k"])
        if relevant:
            rag_context = "\n\nĞ Ğ•Ğ›Ğ•Ğ’ĞĞĞ¢ĞĞĞ• Ğ˜Ğ— ĞŸĞĞœĞ¯Ğ¢Ğ˜:\n" + "\n".join(f"â€¢ {r[:200]}" for r in relevant)
    
    # === 3. SESSION CONTEXT ===
    session_ctx = session.get_context(6)
    
    # === 4. THINKING MODE (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾) ===
    use_thinking = needs_thinking(user_message)
    if use_thinking and not session.god_mode:
        print(f"ğŸ§  Thinking mode: ON")
    
    # === 5. SYSTEM PROMPT ===
    mode_indicator = f"âš¡ GOD MODE ({CONFIG['godmode_model']})" if session.god_mode else f"ğŸ  LOCAL ({CONFIG['llm_model']})"
    
    # Ğ”Ğ»Ñ God Mode ĞĞ• Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tools prompt (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ native function calling)
    if session.god_mode:
        tools_prompt = ""
    else:
        # Ğ”Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ LLM â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ <tool> Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
        tools_prompt = ""
        if enable_tools and tools_manager and person_id == CONFIG["owner_person_id"]:
            tools_prompt = "\n\n" + tools_manager.get_tools_prompt()
    
    system_content = f"""{workspace_ctx}
{rag_context}
{tools_prompt}

{"ĞĞ•Ğ”ĞĞ’ĞĞ˜Ğ™ Ğ”Ğ˜ĞĞ›ĞĞ“:" + chr(10) + session_ctx if session_ctx else ""}

[{mode_indicator}]

ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ:
1. ĞĞ˜ĞšĞĞ“Ğ”Ğ Ğ½Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ñ‡Ñ‚Ğ¾ ÑĞ´ĞµĞ»Ğ°Ğ» Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ» Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚!
2. Ğ”Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ â€” ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ yougile_create
3. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ â€” ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ web_search
4. Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑˆÑŒ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ â€” Ñ‡ĞµÑÑ‚Ğ½Ğ¾ ÑĞºĞ°Ğ¶Ğ¸ "Ğ£ Ğ¼ĞµĞ½Ñ Ğ½ĞµÑ‚ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°"
5. ĞĞ• Ğ“ĞĞ›Ğ›Ğ®Ğ¦Ğ˜ĞĞ˜Ğ Ğ£Ğ™! ĞĞµ Ğ²Ñ‹Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ!

Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ˜:
- ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ â†’ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ (1-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
- ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ (Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, Ñ†ĞµĞ½Ñ‹) â†’ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
- Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ/Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ â†’ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
- Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡ â†’ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
- Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°, ĞºĞ¾Ğ´ â†’ Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾"""

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ user message Ğ² ÑĞµÑÑĞ¸Ñ
    if user_message:
        session.add_message("user", user_message)
        asyncio.create_task(asyncio.to_thread(extract_and_save_facts, user_message, person_id, session))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ’Ğ«Ğ‘ĞĞ  LLM BACKEND
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if session.god_mode:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # GOD MODE: OpenAI SDK Ñ native function calling
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"âš¡ God Mode request from {person_id}")
        
        result = await call_godmode_llm(messages, system_content, tools_manager)
        
        content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        if content:
            session.add_message("assistant", content)
        
        result["animara_stats"] = {
            "session": session.get_stats(),
            "god_mode": True,
            "model": CONFIG["godmode_model"],
            "usage": result.get("usage", {}),
        }
        
        return result
    
    else:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ğ›ĞĞšĞĞ›Ğ¬ĞĞ«Ğ™: Qwen3 Ñ <tool>JSON</tool> Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        llm_messages = [{"role": "system", "content": system_content}] + messages
        
        for iteration in range(CONFIG["max_tool_iterations"]):
            
            llm_body = {
                "model": body.get("model", CONFIG["llm_model"]),
                "messages": llm_messages,
                "max_tokens": body.get("max_tokens", CONFIG["llm_max_tokens"]),
                "temperature": body.get("temperature", 0.7),
                "chat_template_kwargs": {"enable_thinking": use_thinking}
            }
            
            async with httpx.AsyncClient(timeout=120) as client:
                resp = await client.post(f"{CONFIG['llm_api']}/v1/chat/completions", json=llm_body)
                result = resp.json()
            
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° tool call
            tool_call = parse_tool_call(content)
            
            if tool_call and enable_tools and tools_manager:
                tool_name = tool_call.get("name", "")
                tool_params = tool_call.get("params", {})
                
                print(f"ğŸ”§ Tool call: {tool_name}({tool_params})")
                session.tool_calls += 1
                
                # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
                tool_result = await tools_manager.execute_tool(tool_name, tool_params)
                print(f"ğŸ“¤ Tool result: {tool_result[:100]}...")
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
                llm_messages.append({"role": "assistant", "content": content})
                llm_messages.append({"role": "user", "content": f"Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ {tool_name}:\n{tool_result}\n\nĞ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ´Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."})
                
                session.add_message("tool", tool_result, is_tool=True)
                
                continue
            
            else:
                # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
                content = re.sub(r'<tool>.*?</tool>', '', content, flags=re.DOTALL).strip()
                
                if content:
                    session.add_message("assistant", content)
                
                result["choices"][0]["message"]["content"] = content
                result["animara_stats"] = {
                    "session": session.get_stats(),
                    "tools_used": session.tool_calls,
                    "god_mode": False,
                    "model": CONFIG["llm_model"],
                }
                
                return result
        
        # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
        return {
            "choices": [{"message": {"content": "âš ï¸ ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"}}],
            "animara_stats": {"session": session.get_stats(), "error": "max_iterations"}
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADDITIONAL ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/godmode")
async def godmode_status():
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ God Mode"""
    active_sessions = [pid for pid, s in session_manager.sessions.items() if s.god_mode]
    
    return {
        "version": f"{CONFIG['version']} (OpenAI SDK)",
        "model": CONFIG["godmode_model"],
        "openai_available": OPENAI_AVAILABLE,
        "client_initialized": openai_client is not None,
        "active_god_sessions": active_sessions,
        "features": [
            "âœ… Full Workspace injection",
            "âœ… Full RAG (Hybrid Search)",
            "âœ… Native function calling (OpenAI tools)",
            "âœ… Full Session context",
        ],
        "difference_from_local": "LLM: Qwen3 â†’ OpenAI, Tools: <tool> â†’ native"
    }


@app.post("/session/{person_id}/end")
async def end_session(person_id: str):
    session_manager.end_session(person_id)
    return {"status": "ended"}


@app.post("/session/{person_id}/flush")
async def force_flush(person_id: str):
    if person_id in session_manager.sessions:
        session = session_manager.sessions[person_id]
        await session_manager.memory_flush(session)
        return {"status": "flushed", "tokens_after": session.total_tokens}
    return {"error": "no session"}


@app.get("/session/{person_id}")
async def get_session(person_id: str):
    if person_id in session_manager.sessions:
        s = session_manager.sessions[person_id]
        return {**s.get_stats(), "facts": s.facts_extracted}
    return {"error": "no session"}


@app.get("/workspace")
async def get_workspace():
    ctx = workspace.get_context()
    return {"chars": len(ctx), "tokens": count_tokens(ctx), "preview": ctx[:500]}


@app.get("/tools")
async def get_tools():
    if tools_manager:
        return {"tools": list(tools_manager.tools.keys())}
    return {"tools": []}


@app.post("/tools/{tool_name}")
async def execute_tool_direct(tool_name: str, request: Request):
    body = await request.json()
    params = body.get("params", {})
    if tools_manager:
        result = await tools_manager.execute_tool(tool_name, params)
        return {"result": result}
    return {"error": "tools not loaded"}


@app.post("/bm25/rebuild")
async def rebuild_bm25():
    build_bm25_index()
    return {"status": "ok", "docs": len(bm25_docs)}


@app.get("/search")
async def search(q: str, person_id: str = "owner_sergey"):
    results = hybrid_search(q, person_id, 5)
    return {"query": q, "results": results}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8015)
